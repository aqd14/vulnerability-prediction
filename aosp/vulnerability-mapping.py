'''
Scrape Tomcat requirements and vulnerabilities
'''

import re
from contextlib import closing

import pandas as pd
import requests
from bs4 import BeautifulSoup
from requests.exceptions import RequestException
import time
from datetime import datetime
from dateutil import tz


class VulnerabilityScraper:


    SECURITY_ADVISORY_LINKS= ['https://source.android.com/security/bulletin/2019','https://source.android.com/security/bulletin/2018',
                              'https://source.android.com/security/bulletin/2017','https://source.android.com/security/bulletin/2016',
                              'https://source.android.com/security/bulletin/2015']

    ADVISORY_YEARS=['2019','2018','2017','2016','2015']

    # SECURITY_ADVISORY_LINKS = ['https://source.android.com/security/bulletin/pixel/2019',
    #                            'https://source.android.com/security/bulletin/pixel/2018',
    #                            'https://source.android.com/security/bulletin/pixel/2017']
    #
    # ADVISORY_YEARS = ['2019', '2018', '2017']

    CVE_DETAILS_LINK='https://www.cvedetails.com/cve-details.php?cve_id='

    def simple_get(self, url):
        """
        Attempts to get the content at `url` by making an HTTP GET request.
        If the content-type of response is some kind of HTML/XML, return the
        text content, otherwise return None.
        """
        try:
            with closing(requests.get(url, stream=True)) as resp:
                if self.is_good_response(resp):
                    return resp.content
                else:
                    return None

        except RequestException as e:
            self.log_error('Error during requests to {0} : {1}'.format(url, str(e)))
            return None

    def is_good_response(self, resp):
        """
        Returns True if the response seems to be HTML, False otherwise.
        """
        content_type = resp.headers['Content-Type'].lower()
        return (resp.status_code == 200
                and content_type is not None
                and content_type.find('html') > -1)

    def log_error(self, e):
        """
        It is always a good idea to log errors.
        This function just prints them, but you can
        make it do anything.
        """
        print(e)

    def get_advisory(self, url):
        response = self.simple_get(url)
        # soup = BeautifulSoup(response, 'lxml')
        soup = BeautifulSoup(response, 'html.parser')

        a=soup.find(id='gc-wrapper').find_all("div", attrs={"class":"devsite-article-body"})[0];

        a = a.table;
        trs=a.find_all('tr', recursive=False);

        dfs = []
        for ix in range(1,len(trs)):
            try:
                dfs.append(self.process_advisory(trs[ix].find_all('td',recursive=False)[0].a['href']))
            except:
                print("Couldn't find link")
            # dfs.append(self.parse_div(divs[ix]))

        return pd.concat(dfs)

    def process_advisory(self, link):

        severity=[]
        title=[]
        cve_id=[]
        description=[]
        revision=[]
        cwe_id=[]
        modified_file=[]
        change_date=[]
        revision_link=[]

        response = self.simple_get(link)
        soup = BeautifulSoup(response, 'html.parser')

        a = soup.find(id='gc-wrapper').find_all("div", attrs={"class": "devsite-article-body"})[0];

        # a = a.find_all('table',recursive=False);
        trs = a.find_all('tr', recursive=True);

        for ix, tr in enumerate(trs):
            if tr.find('td'):
                tds=tr.find_all('td',recursive=False)
                if len(tds)>3 and tds[1].find('a') and tds[1].find('a')['href']!='#asterisk':

                    cid=self.process_text(tds[0].text);
                    if re.search('^cve-[0-9]+-[0-9]+$',cid)!=None:
                        cve_id.append(cid)

                        if tds[0].has_attr('rowspan'):
                            print(cid+' [rowspan]');

                        anchors=tds[1].find_all('a');
                        mf='';
                        md=''

                        anchors_str=''

                        for a in anchors:
                            cf,cd= self.get_modified_files(a['href'])
                            if cf!='':
                                mf+=cf
                                md=cd
                            anchors_str+=a['href']+'|';

                        anchors_str=anchors_str[:-1]
                        mf = mf[:-1]
                        s = set()
                        fs = mf.split(';')
                        for f in fs:
                            s.add(f);
                        mf = ''
                        for f in s:
                            mf += f + ';'
                        mf = mf[:-1]

                        modified_file.append(mf);
                        change_date.append(md)
                        revision_link.append(anchors_str)
                        cwe_id.append(self.get_cwe_id(cid))

        df = pd.DataFrame({
                # 'severity': severity,
                # 'title': title,
                'cve id': cve_id,
                # 'revision': revision,
                 'cwe id': cwe_id,
                'modified file': modified_file,
                'change date': change_date,
                'revision link': revision_link
        })
        return df

    def get_cwe_id(self,cve_id):
        cwe_id = ''
        try:
            response = self.simple_get(self.CVE_DETAILS_LINK+cve_id)
            soup = BeautifulSoup(response, 'lxml')
            trs=soup.find(id='cvssscorestable').find_all('tr', recursive=False)

            for ix, ct in enumerate(trs):
                if self.process_text(ct.find('th').text)=='cwe id' and ct.find('td').find('a'):
                    cwe_id=(self.process_text(ct.find('td').find('a').text))

            if cwe_id=='':
                raise Exception();
        except:
            print('No CWE ID found for '+cve_id)

        return cwe_id

    def get_modified_files(self,rev_link):

        if rev_link.startswith('http')==False:
            rev_link="https:"+rev_link;

        repoWorked=False;
        m_files = ''
        change_date=''
        try:
            m_files,change_date=self.try_android_repo(rev_link);
            repoWorked = True;
        except:
            pass
            # print('Android repo did not work ' +rev_link)

        if repoWorked==False:
            try:
                m_files,change_date = self.try_kernel_repo(rev_link);
                repoWorked = True;
            except:
                pass
                # print('Kernel repo did not work ' + rev_link)

        if repoWorked==False:
            try:
                m_files, change_date= self.try_patch_repo(rev_link);
                repoWorked = True;
            except:
                pass
                # print('Kernel repo did not work ' + rev_link)

        if repoWorked==False:
            try:
                m_files, change_date= self.try_github_repo(rev_link);
                repoWorked = True;
            except:
                pass
                # print('Kernel repo did not work ' + rev_link)

        if repoWorked==False:
            print('No repo worked for '+rev_link)
        elif m_files!=''and change_date=='':
            print('Not able to find datetime for ' + rev_link)

        return m_files,change_date

    def try_android_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        ul=soup.find_all("ul", attrs={"class": "DiffTree"})[0];
        lis=ul.find_all("li", recursive=False);

        for ix, li in enumerate(lis):

            if li.find('a'):
                # print(li.find('a').text)
                path=self.clean_path(li.find('a').text);
                if self.isValidFile(path):
                    m_files += path + ';'

        if m_files=='':
            raise Exception();

        change_date=''

        try:
            div = soup.find_all("div", attrs={"class": "Metadata"})[0];
            trs = div.find_all("tr");


            for ix, tr in enumerate(trs):

                if tr.find('th') and self.process_text(tr.find('th').text)=='committer'\
                        and len(tr.find_all('td'))>1:
                    change_date=self.process_text(tr.find_all('td')[1].text)
                    break;

            datetime_object = datetime.strptime(change_date, '%a %b %d %H:%M:%S %Y %z')
            to_zone = tz.tzlocal()
            change_date=datetime_object.astimezone(to_zone).strftime('%a %b %d %H:%M:%S %Y %z')
        except:
            pass
            # print('Not able to find datetime for '+rev_link)

        if m_files == '':
            raise Exception();

        return m_files[:-1],change_date

    def try_kernel_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        tds=soup.find_all("td", attrs={"class": ["upd","del"]});

        for ix, td in enumerate(tds):

            if td.find('a'):
                # print(td.find('a').text)
                path=self.clean_path(td.find('a').text);
                if self.isValidFile(path):
                    m_files += path + ';'

        change_date = ''

        try:
            div = soup.find_all("table", attrs={"class": "commit-info"})[0];
            trs = div.find_all("tr");

            for ix, tr in enumerate(trs):

                if tr.find('th') and self.process_text(tr.find('th').text) == 'committer' \
                        and len(tr.find_all('td')) > 1:
                    change_date = self.process_text(tr.find_all('td')[1].text)
                    break;

            datetime_object = datetime.strptime(change_date, '%Y-%m-%d %H:%M:%S %z')
            to_zone = tz.tzlocal()
            change_date = datetime_object.astimezone(to_zone).strftime('%a %b %d %H:%M:%S %Y %z')

        except:
            pass
            # print('Not able to find datetime for ' + rev_link)

        if m_files=='':
            raise Exception();

        return m_files[:-1],change_date

    def try_patch_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        div=soup.find('div',id='patch');
        spans=div.find_all('span', attrs={"class": "p_header"});


        for ix, span in enumerate(spans):
            t=self.process_text(span.text);
            if t.startswith('diff --git'):
                ts=t.split();
                path=ts[len(ts)-1][2:];
                # print(path)
                if self.isValidFile(path):
                    m_files += path + ';'

        change_date = ''

        try:
            div = soup.find_all("div", attrs={"class": "meta"})[0];

            if len(div.find_all('span')) > 1:
                change_date = self.process_text(div.find_all('span')[1].text)

                change_date=change_date.replace('.','');
                change_date=change_date.replace('utc','');
                change_date+=' +0000';
                datetime_object = datetime.strptime(change_date, '%b %d, %Y, %I:%M %p %z')
                to_zone = tz.tzlocal()
                change_date = datetime_object.astimezone(to_zone).strftime('%a %b %d %H:%M:%S %Y %z')

        except:
            pass
            # print('Not able to find datetime for ' + rev_link)


        if m_files=='':
            raise Exception();

        return m_files[:-1],change_date

    def try_github_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        div=soup.find('div',id='files');
        divs=div.find_all('div', attrs={"class": "file-info"});

        for ix, d in enumerate(divs):
            if d.find('a'):
                # print(d.find('a').text)
                path=self.clean_path(d.find('a').text);
                if self.isValidFile(path):
                    m_files += path + ';';

        change_date = ''

        try:
            div = soup.find_all("div", attrs={"class": "commit-meta"})[0];
            trs = div.find_all("div");

            for ix, tr in enumerate(trs):

                if tr.find('relative-time'):
                    change_date = self.process_text(tr.find('relative-time')['datetime'])
                    break;

            change_date=change_date.replace('z',' +0000')
            datetime_object = datetime.strptime(change_date, '%Y-%m-%dt%H:%M:%S %z')
            to_zone = tz.tzlocal()
            change_date = datetime_object.astimezone(to_zone).strftime('%a %b %d %H:%M:%S %Y %z')

        except:
            pass
            # print('Not able to find datetime for ' + rev_link)

        if m_files=='':
            raise Exception();

        return m_files[:-1],change_date

    def isValidFile(self,path):
        ps=path.split('/')

        # if '.' not in ps[len(ps)-1]:
        #     return False

        # if 'changelog.xml'== ps[len(ps)-1].lower():
        #     return False
        #
        # if 'localstrings.properties'== ps[len(ps)-1].lower():
        #     return False
        # if 'mbeans-descriptors.xml'== ps[len(ps)-1].lower():
        #     return False

        for p in ps:
            if 'test'==p.lower() or 'tests'==p.lower():
                return False;

        return True

    def clean_path(self,path):
        path = path.strip()
        return path

    def process_text(self,str):
        str=str.lower()
        str=str.strip()
        return str

    def startsWith(self, wid, text):
        l=text.split()
        for s in l:
            if s.startswith(wid):
                return True
        return False


parser = VulnerabilityScraper()
for index in range(len(parser.SECURITY_ADVISORY_LINKS)):

    print('Retrieving mapping for the year '+parser.ADVISORY_YEARS[index]);

    df = parser.get_advisory(parser.SECURITY_ADVISORY_LINKS[index])
    # df = parser.process_advisory('https://source.android.com/security/bulletin/2016-01-01')
    df.to_csv('aosp_pixel_vul_mapping_'+parser.ADVISORY_YEARS[index]+'.csv', sep=',', index=False, encoding='utf-8')

    print('Scraping done for the year '+parser.ADVISORY_YEARS[index]);

    time.sleep(10);

# parser.process_advisory('https://source.android.com/security/bulletin/2019');

# print(parser.try_kernel_repo('https://source.codeaurora.org/quic/la/kernel/msm-3.18/commit/?id=9f261e5dfe101bbe35043822a89bffa78e080b3b'));

# print(parser.try_android_repo('https://android.googlesource.com/platform/system/security/+/f8feed620bd607427ded702cce91bb0eb749bc6a'))

# print(parser.try_patch_repo('https://lore.kernel.org/patchwork/patch/1040325/'))

# print(parser.try_github_repo('https://github.com/torvalds/linux/commit/b5a663aa426f4884c71cd8580adae73f33570f0d'))

# print(parser.get_cwe_id('cve-2015-6631'))