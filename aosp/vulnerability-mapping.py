'''
Scrape Tomcat requirements and vulnerabilities
'''

import re
from contextlib import closing

import pandas as pd
import requests
from bs4 import BeautifulSoup
from requests.exceptions import RequestException
import time


class VulnerabilityScraper:


    # SECURITY_ADVISORY_LINKS= ['https://source.android.com/security/bulletin/2019','https://source.android.com/security/bulletin/2018',
    #                           'https://source.android.com/security/bulletin/2017','https://source.android.com/security/bulletin/2016',
    #                           'https://source.android.com/security/bulletin/2015']
    #
    # ADVISORY_YEARS=['2019','2018','2017','2016','2015']

    SECURITY_ADVISORY_LINKS = ['https://source.android.com/security/bulletin/pixel/2019',
                               'https://source.android.com/security/bulletin/pixel/2018',
                               'https://source.android.com/security/bulletin/pixel/2017']

    ADVISORY_YEARS = ['2019', '2018', '2017']

    CVE_DETAILS_LINK='https://www.cvedetails.com/cve-details.php?cve_id='

    def simple_get(self, url):
        """
        Attempts to get the content at `url` by making an HTTP GET request.
        If the content-type of response is some kind of HTML/XML, return the
        text content, otherwise return None.
        """
        try:
            with closing(requests.get(url, stream=True)) as resp:
                if self.is_good_response(resp):
                    return resp.content
                else:
                    return None

        except RequestException as e:
            self.log_error('Error during requests to {0} : {1}'.format(url, str(e)))
            return None

    def is_good_response(self, resp):
        """
        Returns True if the response seems to be HTML, False otherwise.
        """
        content_type = resp.headers['Content-Type'].lower()
        return (resp.status_code == 200
                and content_type is not None
                and content_type.find('html') > -1)

    def log_error(self, e):
        """
        It is always a good idea to log errors.
        This function just prints them, but you can
        make it do anything.
        """
        print(e)

    def get_advisory(self, url):
        response = self.simple_get(url)
        # soup = BeautifulSoup(response, 'lxml')
        soup = BeautifulSoup(response, 'html.parser')

        a=soup.find(id='gc-wrapper').find_all("div", attrs={"class":"devsite-article-body"})[0];

        a = a.table;
        trs=a.find_all('tr', recursive=False);

        dfs = []
        for ix in range(1,len(trs)):
            try:
                dfs.append(self.process_advisory(trs[ix].find_all('td',recursive=False)[0].a['href']))
            except:
                print("Couldn't find link")
            # dfs.append(self.parse_div(divs[ix]))

        return pd.concat(dfs)

    def process_advisory(self, link):

        severity=[]
        title=[]
        cve_id=[]
        description=[]
        revision=[]
        cwe_id=[]
        modified_file=[]

        response = self.simple_get(link)
        soup = BeautifulSoup(response, 'html.parser')

        a = soup.find(id='gc-wrapper').find_all("div", attrs={"class": "devsite-article-body"})[0];

        # a = a.find_all('table',recursive=False);
        trs = a.find_all('tr', recursive=True);

        for ix, tr in enumerate(trs):
            if tr.find('td'):
                tds=tr.find_all('td',recursive=False)
                if len(tds)>3 and tds[1].find('a') and tds[1].find('a')['href']!='#asterisk':

                    cid=self.process_text(tds[0].text);
                    if re.search('^cve-[0-9]+-[0-9]+$',cid)!=None:
                        cve_id.append(cid)

                        if tds[0].has_attr('rowspan'):
                            print(cid+' [rowspan]');

                        anchors=tds[1].find_all('a');
                        mf='';
                        for a in anchors:
                            mf+=self.get_modified_files(a['href'])+';'

                        mf = mf[:-1]
                        s = set()
                        fs = mf.split(';')
                        for f in fs:
                            s.add(f);
                        mf = ''
                        for f in s:
                            mf += f + ';'
                        mf = mf[:-1]

                        modified_file.append(mf);

        df = pd.DataFrame({
                # 'severity': severity,
                # 'title': title,
                'cve id': cve_id,
                # 'revision': revision,
                # 'cwe id': cwe_id,
                'modified file': modified_file
        })
        return df

    def get_cwe_id(self,cve_id):
        cwe_id = ''
        try:
            response = self.simple_get(self.CVE_DETAILS_LINK+cve_id)
            soup = BeautifulSoup(response, 'lxml')
            trs=soup.find(id='cvssscorestable').find_all('tr', recursive=False)

            for ix, ct in enumerate(trs):
                if self.process_text(ct.find('th').text)=='cwe id' and ct.find('td').find('a'):
                    cwe_id=(self.process_text(ct.find('td').find('a').text))

            if cwe_id=='':
                raise Exception();
        except:
            print('No CWE ID found for '+cve_id)

        return cwe_id

    def get_modified_files(self,rev_link):

        if rev_link.startswith('http')==False:
            rev_link="https:"+rev_link;

        repoWorked=False;
        m_files = ''
        try:
            m_files+=self.try_android_repo(rev_link);
            repoWorked = True;
        except:
            pass
            # print('Android repo did not work ' +rev_link)

        if repoWorked==False:
            try:
                m_files += self.try_kernel_repo(rev_link);
                repoWorked = True;
            except:
                pass
                # print('Kernel repo did not work ' + rev_link)

        if repoWorked==False:
            try:
                m_files += self.try_patch_repo(rev_link);
                repoWorked = True;
            except:
                pass
                # print('Kernel repo did not work ' + rev_link)

        if repoWorked==False:
            try:
                m_files += self.try_github_repo(rev_link);
                repoWorked = True;
            except:
                pass
                # print('Kernel repo did not work ' + rev_link)

        if repoWorked==False:
            print('No repo worked for '+rev_link)

        return m_files

    def try_android_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        ul=soup.find_all("ul", attrs={"class": "DiffTree"})[0];
        lis=ul.find_all("li", recursive=False);

        for ix, li in enumerate(lis):

            if li.find('a'):
                # print(li.find('a').text)
                path=self.clean_path(li.find('a').text);
                if self.isValidFile(path):
                    m_files += path + ';'

        if m_files=='':
            raise Exception();

        return m_files[:-1]

    def try_kernel_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        tds=soup.find_all("td", attrs={"class": ["upd","del"]});

        for ix, td in enumerate(tds):

            if td.find('a'):
                # print(td.find('a').text)
                path=self.clean_path(td.find('a').text);
                if self.isValidFile(path):
                    m_files += path + ';'

        if m_files=='':
            raise Exception();

        return m_files[:-1]

    def try_patch_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        div=soup.find('div',id='patch');
        spans=div.find_all('span', attrs={"class": "p_header"});


        for ix, span in enumerate(spans):
            t=self.process_text(span.text);
            if t.startswith('diff --git'):
                ts=t.split();
                path=ts[len(ts)-1][2:];
                # print(path)
                if self.isValidFile(path):
                    m_files += path + ';'



        if m_files=='':
            raise Exception();

        return m_files[:-1]

    def try_github_repo(self,rev_link):

        m_files = ''

        response = self.simple_get(rev_link);
        soup = BeautifulSoup(response, 'html.parser');
        div=soup.find('div',id='files');
        divs=div.find_all('div', attrs={"class": "file-info"});

        for ix, d in enumerate(divs):
            if d.find('a'):
                # print(d.find('a').text)
                path=self.clean_path(d.find('a').text);
                if self.isValidFile(path):
                    m_files += path + ';';

        if m_files=='':
            raise Exception();

        return m_files[:-1]

    def isValidFile(self,path):
        ps=path.split('/')

        # if '.' not in ps[len(ps)-1]:
        #     return False

        # if 'changelog.xml'== ps[len(ps)-1].lower():
        #     return False
        #
        # if 'localstrings.properties'== ps[len(ps)-1].lower():
        #     return False
        # if 'mbeans-descriptors.xml'== ps[len(ps)-1].lower():
        #     return False

        for p in ps:
            if 'test'==p.lower() or 'tests'==p.lower():
                return False;

        return True

    def clean_path(self,path):
        path = path.strip()
        return path

    def process_text(self,str):
        str=str.lower()
        str=str.strip()
        return str

    def startsWith(self, wid, text):
        l=text.split()
        for s in l:
            if s.startswith(wid):
                return True
        return False


parser = VulnerabilityScraper()
for index in range(len(parser.SECURITY_ADVISORY_LINKS)):

    print('Retrieving mapping for the year '+parser.ADVISORY_YEARS[index]);

    df = parser.get_advisory(parser.SECURITY_ADVISORY_LINKS[index])
    # df = parser.process_advisory('https://source.android.com/security/bulletin/2016-01-01')
    df.to_csv('aosp_pixel_vul_mapping_'+parser.ADVISORY_YEARS[index]+'.csv', sep=',', index=False, encoding='utf-8')

    print('Scraping done for the year '+parser.ADVISORY_YEARS[index]);

    time.sleep(10);

# print(parser.try_kernel_repo('https://source.codeaurora.org/quic/la/kernel/msm-3.18/commit/?id=dc333eb1c31b5bdd2b6375d7cb890086d8f27d8b'));