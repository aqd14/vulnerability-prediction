'''
Collect requirements to source files mappings
'''

import os
import pandas as pd
from bs4 import BeautifulSoup
import requests
from time import sleep

URL_PREFIX = 'https://bugzilla.mozilla.org/show_bug.cgi?id={}'

df = pd.read_csv(os.path.join(os.getcwd(), 'data', 'bugid2commit.csv'))[1000:10000]

titles = []
fixed_versions = []
assignees = []
fixed_dates = []
descriptions = []
reporters = []


def scrape(url):
    """
    scraping firefox bug details from Bugzilla
    :param url: bug url
    :return: list of bug details
    """

    title = None
    version = None
    assignee = None
    date = None
    description = None
    reporter = None

    print('scraping %s' % url)
    try:
        soup = BeautifulSoup(requests.get(url).content, 'lxml')
        if soup.find(id='docslinks') is not None or \
                'FIXED' not in soup.find(id='field-value-status-view').text:
            return [None] * 6

        title = soup.find(id='field-value-short_desc').text
        version = soup.find(id='field-value-version').text.strip(' \t\n\r')
        date = soup.find(id='field-value-delta_ts').find(class_='rel-time')['data-time'] # timestamp
        description = soup.find(id='ct-0').text.replace(',', '')
        reporter = soup.find(id='field-reporter').find(class_='fna').text
        assignee = soup.find(id='field-assigned_to').find(class_='fna').text

    except Exception as e:
        print(e)
        sleep(5)

    return [title, version, assignee, date, description, reporter]


for i, id in enumerate(df.id):
    print(str(i))
    url = URL_PREFIX.format(id)
    results = scrape(url)
    # extract scraped data
    titles.append(results[0])
    fixed_versions.append(results[1])
    assignees.append(results[2])
    fixed_dates.append(results[3])
    descriptions.append(results[4])
    reporters.append(results[5])

df['title'] = pd.Series(titles, index=df.index)
df['version'] = pd.Series(fixed_versions, index=df.index)
df['assignee'] = pd.Series(assignees, index=df.index)
df['reporter'] = pd.Series(reporters, index=df.index)
df['date'] = pd.Series(fixed_dates, index=df.index)
df['description'] = pd.Series(descriptions, index=df.index)

df.to_csv('data\\req2source_2.csv', index=False)