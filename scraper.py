'''
Scrape Tomcat requirements and vulnerabilities
'''

import re
from contextlib import closing

import pandas as pd
import requests
from bs4 import BeautifulSoup
from requests.exceptions import RequestException


class HTMLTableParser:

    def simple_get(self, url):
        """
        Attempts to get the content at `url` by making an HTTP GET request.
        If the content-type of response is some kind of HTML/XML, return the
        text content, otherwise return None.
        """
        try:
            with closing(requests.get(url, stream=True)) as resp:
                if self.is_good_response(resp):
                    return resp.content
                else:
                    return None

        except RequestException as e:
            self.log_error('Error during requests to {0} : {1}'.format(url, str(e)))
            return None

    def is_good_response(self, resp):
        """
        Returns True if the response seems to be HTML, False otherwise.
        """
        content_type = resp.headers['Content-Type'].lower()
        return (resp.status_code == 200
                and content_type is not None
                and content_type.find('html') > -1)

    def log_error(self, e):
        """
        It is always a good idea to log errors.
        This function just prints them, but you can
        make it do anything.
        """
        print(e)

    # def extract_change_type(self, src):
    #     if 'fix' in src:
    #         return 'fix'
    #     elif 'update' in src:
    #         return 'update'
    #     elif 'add' in src:
    #         return 'add'
    #     else:
    #         return 'NaN'

    def parse_url(self, url):
        response = self.simple_get(url)
        soup = BeautifulSoup(response, 'lxml')
        # return [(table['id'], self.parse_html_table(table))
        #         for table in soup.find_all('table')]

        # each Tomcat version has its own table for showing change logs
        # mainBody = soup.find(id='mainBody')
        version_tables = soup.find(id='mainBody').find_all('table', recursive=False)
        print(len(version_tables))
        # for table in tables:
        dfs = []
        for ix, vs in enumerate(version_tables):
            dfs.append(self.parse_html_table(vs))
            print(ix)

        return pd.concat(dfs)
        # print(df.head())

    def parse_html_table(self, table):
        """
        Parse a HTML table
        :param table: html table to be parsed
        :return: a DataFrame containing all information about the changes
        """

        # columns = ['component', 'type', 'change', 'link']
        components = []
        # versions = []
        types = []
        changes = []
        links = []

        # df = pd.DataFrame(columns=columns, index=range(0, n_rows))
        try:
            version = table.find('tr').find('td').find('a')['name']
        except AttributeError as e:
            print(e)

        change_tables = table.find_all('tr', recursive=False)[1].find('td').find('blockquote').find_all('table', recursive=False)  # each table shows changes for each component

        # inner_tables[0].text
        for ix, ct in enumerate(change_tables):
            # print(ct.prettify())
            inner_rows = ct.find_all('tr', recursive=False)
            # print(inner_rows)
            component = inner_rows[0].text
            # print(component)

            # actual changes description
            try:
                trs = inner_rows[1].find('td').find('blockquote').find('table').find_all('tr')
            except AttributeError as e: # some change logs has a wrong format. This is very infrequent so don't need to make the code complicated
                print(e)
                continue
            for tr in trs:
                components.append(component)
                tds = tr.find_all('td')
                # src = tds[0].find('img')['src']
                # print(src)
                change_type = tds[0].find('img')['alt'] #self.extract_change_type(src)
                types.append(change_type)
                # print(change_type)

                # extract hyperlink to the issue
                link = 'NaN'
                if tds[1].find('a'):
                    link = tds[1].find('a')['href']
                    # print(link)
                # if link != 'NaN'
                links.append(link)
                change = re.sub(' +', ' ', tds[1].text).strip().replace('\n', '')
                # print(change)
                changes.append(change)

        df = pd.DataFrame({
                'component': components,
                'version': [version] * len(changes),
                'type': types,
                'change': changes,
                'link': links
        })
        return df


parser = HTMLTableParser()
df = parser.parse_url('https://tomcat.apache.org/tomcat-7.0-doc/changelog.html')
df.to_csv('tomcat.csv', sep='|', index=False, encoding='utf-8')
