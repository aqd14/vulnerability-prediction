import re
from nltk import PorterStemmer
from nltk import pos_tag
from nltk import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import unicodedata
from tomcat import *
# cases not handled: 5times, helloWORLD, 15
# side-effects: won't would produce won and t
# stemming needs to be done: issues and issue should be treated same

stop_list=stopwords.words('english')
wordnet_lemmatizer = WordNetLemmatizer()

delimiters="."," ","-","_","!",".","=","'",",","\\","(",")","}","{","[","]","^","\"","`",":","?",">","<","/",";","$","~","@","#","*","%","&","+","|"
regex_pattern="|".join(map(re.escape, delimiters))

def cleanSpecialOperators(str):
    for op in special_operators:
        str=str.replace(op, ' ')
    return str;

def removeAscendingChar(data):
    data=unicodedata.normalize('NFKD', data).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return data

def clean_text(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"won't", " will not ", text)
    text = re.sub(r"shan't", " shall not ", text)
    text = re.sub(r"gonna", " going to ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub('\W', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = text.strip(' ')
    text=text.lower()
    return text

def lengthen(word):
    if word in lengthen_keys:
        return lengthen_keys[word];
    return word;

def keepAlpha(sentence):
    alpha_sent = ""
    for word in sentence.split():
        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)
        alpha_sent += alpha_word
        alpha_sent += " "
    alpha_sent = alpha_sent.strip()
    return alpha_sent

def lemitizeWords(word):
    x=wordnet_lemmatizer.lemmatize(word,'v')
    return x

def stemWords(word):
    x=PorterStemmer().stem(word)
    return x

def domainCleanup(str):
    if str in stop_list:
        return False;
    if str in common_keywords:
        return False;
    if str in basic_java_keywords:
        return False;
    if str in project_keywords:
        return False;
    if str in basic_html_keywords:
        return False;

    return True;


def preprocess(data):
    data=cleanSpecialOperators(data)
    data=removeAscendingChar(data)
    data=clean_text(data)
    data=keepAlpha(data)

    refined_terms=[]
    terms=data.split()

    for term in terms:
        term=lengthen(term)
        if(domainCleanup(term)):
            # root_term=lemitizeWords(term)
            root_term=stemWords(term)
            refined_terms.append(root_term)

    return refined_terms



if __name__=='__main__':
    terms=[]
    # cleanHtml("pointing pointed is fo\\ta \"f\", i'm  no    joke!told ya.hi_five=5 hellYa DEF", terms)
    # print (terms)
    # print(cleanHtml("<div id='copra' kemte='6'>hey</div>"))
    print(lengthen('hi'))
    print(clean_text("H/fU I've!won't what's.it'd take's 125 closeNow vir_12 mak12"))
    print(keepAlpha(clean_text("H/fU I've!won't what's.it'd take's 125 closeNow vir_12_uttom mak12")))